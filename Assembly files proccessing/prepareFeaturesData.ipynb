{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5b27ee-635e-4e17-99ef-e4485e0312fd",
   "metadata": {},
   "source": [
    "# Preparing Data\n",
    "\n",
    "To group our data together we must now the ID of each file.\n",
    "In this part, we create a name list like previous, and use this name list to create a **ID** column in each CSV file filled with the name of file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2d0f23-7553-42d9-8d68-52bfb128a15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/07 17:32:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from operator import add\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "from pyspark.sql import functions as func\n",
    "\n",
    "sc = SparkContext()\n",
    "spark = SparkSession.builder.master(\"master[*]\").appName(\"MalwareClassification - prepare data\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c99ed96-e126-4942-a288-a7772e746144",
   "metadata": {},
   "source": [
    "In this part, we create a name list from **trainLabels.csv** like before. Then we create the **ID** column and save the csv file in a folder called **featuresWithID**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6057f8ae-9dc6-4eab-92cb-d2e793465cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "trainFilenamesArray = np.genfromtxt('trainLabels.csv', delimiter=\",\", dtype=None, encoding=None)\n",
    "trainFilenamesArray = trainFilenamesArray[1:].tolist()\n",
    "trainFilenamesList = [item[0] for item in trainFilenamesArray]\n",
    "trainFilenamesList = [item.replace('\"', \"\") for item in trainFilenamesList]\n",
    "for name in trainFilenamesList:\n",
    "    csvDf = spark.read.option(\"header\",True).csv(f'features/{name}.csv')\n",
    "    newcsv = csvDf.withColumn(\"_c0\", lit(name))\n",
    "    newcsv.toPandas().to_csv(f'featuresWithID/{name}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91130550-9141-4f3d-b03e-1ddcb71ad79b",
   "metadata": {},
   "source": [
    "# Create feature Dataset\n",
    "In spark, when you read some CSV file together, it creates a **data frame** from the files. Here we reads all CSV files in **featuresWithID** folder and save it in a Data Frame called `featuresDataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbe59473-1d7d-4673-abf9-534172963a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "featuresDataFrame = spark.read.options(header=True, inferSchema=True).csv(\"./featuresWithID/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7808576-f520-47ad-9421-b05aba6599cd",
   "metadata": {},
   "source": [
    "Then we group the entire data frame by their ID's and use `Pivot` method to create a column of IDs and a row contained of Opcodes. This will create a table that filled with count of Opcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b44ead6-fa45-49ec-a647-94f2a67d39b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "featuresDataFramePivot = featuresDataFrame.groupby(\"_c0\").pivot(\"OPcodes\").max(\"count\").fillna(0)\n",
    "featuresDataFramePivot = featuresDataFramePivot.withColumnRenamed(\"_c0\",\"ID\")\n",
    "featuresDataFramePivot.toPandas().to_csv(\"featureFrequency.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c61831-c2cb-4536-8c35-30a1014a9341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
