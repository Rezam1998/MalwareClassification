{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c40918e0-018b-446b-b647-d012554db331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import re\n",
    "import numpy as np\n",
    "from operator import add\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a23c7e5-671c-4a40-b77d-082e5a66f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPCODE_PATTERN = re.compile(r'([\\s])([A-F0-9]{2})([\\s]+)([a-z]+)([\\s+])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fb7cce7-a005-4353-86a3-02da4e6a716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_folder_path, filenames, type):\n",
    "    myRDDlist = []\n",
    "    for filename in filenames:\n",
    "        new_rdd = sc.textFile(data_folder_path +\"/\"+ filename + type).map(lambda x: (filename,x)).groupByKey().map(lambda x: (x[0],' '.join(x[1])))\n",
    "        myRDDlist.append(new_rdd)\n",
    "    Spark_Full = sc.union(myRDDlist)\n",
    "    return Spark_Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "177ebffd-950f-4d18-9bc6-23807cededc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_label_pair(filenames_data_rdd,labels_rdd):\n",
    "    \"\"\"\n",
    "        This function matches the filename with label\n",
    "        \n",
    "        --input-------------------------------------\n",
    "        filenames_data_rdd : [<hash1>, <hash2>, ...]\n",
    "        labels_rdd : [label1, label2, ...]\n",
    "        \n",
    "        --output------------------------------------\n",
    "        filename_label_pair : [(<hash1>,<label1>), (<hash2>,<label2>), ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    id_filenames_rdd = filenames_data_rdd.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "    id_label_rdd = labels_rdd.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "    filename_label_pair = id_filenames_rdd.join(id_label_rdd).map(lambda x: x[1])\n",
    "    return filename_label_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd76bd95-be32-427d-95de-89936072be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_rdd, feature_name):\n",
    "    \"\"\"\n",
    "        This function extracts the required features\n",
    "        \n",
    "        --input-------------------------------------\n",
    "        file_rdd : [(<hash1>, <content1>), ...]\n",
    "        feature_name : str\n",
    "        \n",
    "        --output------------------------------------\n",
    "        filename_label_pair : [(<hash1>,<feature1>), (<hash1>,<feature2>), ..., (<hashN>,<featureK>)]\n",
    "    \"\"\"\n",
    "    \n",
    "    if feature_name=='bytes':\n",
    "        return file_rdd.map(lambda x: (x[0],BYTES_PATTERN.findall(x[1]))).flatMapValues(lambda x:x)\n",
    "    elif feature_name=='segment':\n",
    "        return file_rdd.map(lambda x: (x[0],SEGMENT_PATTERN.findall(x[1]))).flatMapValues(lambda x:x)\n",
    "    elif feature_name=='opcode':\n",
    "        return file_rdd.map(lambda x: (x[0],OPCODE_PATTERN.findall(x[1]))).flatMapValues(lambda x:x).map(lambda x: (x[0],x[1][3]))\n",
    "    else:\n",
    "        return \"Invalid input!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4051459e-752e-460d-96c6-dff1ac371d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ngram(feature_rdd,start,end):\n",
    "    '''\n",
    "        --input-------------------------------------\n",
    "        feature_rdd : [(<hash1>,<feature1>), (<hash1>,<feature2>), ..., (<hashN>,<featureK>)]\n",
    "        \n",
    "        --output------------------------------------\n",
    "        Ngram_count : [((<hash>,<ngram feature>),cnt), ...]\n",
    "        '''\n",
    "    Ngram_list = []\n",
    "    for i in range(start,end):\n",
    "        Ngram_list.append(Ngram_feature(i, feature_rdd))\n",
    "    Ngram_count = sc.union(Ngram_list)\n",
    "    return Ngram_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80e3ab49-b2c4-4b0a-a761-ee63cd802b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ngram_feature(N, feature_rdd):\n",
    "    '''\n",
    "        Extract and count N-gram. Leave top 1000 n-gram features if it's 2-gram or more.\n",
    "        \n",
    "        Input:\n",
    "        feature_rdd : [(<hash1>,<feature1>), (<hash1>,<feature2>), ..., (<hashN>,<featureK>)]\n",
    "        \n",
    "        Output:\n",
    "        freq_ngram_count_rdd : [((<hash>,<ngram feature>),cnt), ...]\n",
    "        '''\n",
    "    feature_rdd = feature_rdd.groupByKey().map(lambda x: (x[0],list(x[1])))\n",
    "    df = spark.createDataFrame(feature_rdd).toDF(\"file_names\", \"features\")\n",
    "    ngram = NGram(n=N, inputCol=\"features\", outputCol=\"ngrams\")\n",
    "    ngramDataFrame = ngram.transform(df)\n",
    "    ngram_rdd = ngramDataFrame.rdd.map(tuple).map(lambda x: (x[0],x[2])).flatMapValues(lambda x: x)\n",
    "    ngram_count_rdd = ngram_rdd.map(lambda x: ((x),1)).reduceByKey(add)\n",
    "    freq_ngram_count_rdd = ngram_count_rdd\n",
    "\n",
    "    if not N == 1:\n",
    "        #[(<ngram feature>,cnt), ...]\n",
    "        topN_ngram_count_rdd = freq_ngram_count_rdd.map(lambda x: (x[0][1],x[1])).reduceByKey(add)\n",
    "        #[((<ngram feature>,cnt),index), ...]\n",
    "        topN_ngram_count_rdd = topN_ngram_count_rdd.sortBy(lambda x: x[1],ascending=False).zipWithIndex()\n",
    "        length = topN_ngram_count_rdd.count()\n",
    "        #top [(<ngram feature>,cntSum), ...]\n",
    "        topN_ngram_count_rdd = topN_ngram_count_rdd.filter(lambda x: x[1]<1000).map(lambda x: x[0])\n",
    "        #freq [(<ngram feature>,(<hash>,cnt)), ...]\n",
    "        freq_ngram_count_rdd = freq_ngram_count_rdd.map(lambda x: (x[0][1],(x[0][0],x[1])))\n",
    "        #[(<ngram feature>,(cntSum,(<hash>,cnt))), ...]\n",
    "        freq_ngram_count_rdd = topN_ngram_count_rdd.join(freq_ngram_count_rdd).map(lambda x: ((x[1][1][0],x[0]),x[1][1][1]))\n",
    "    \n",
    "    return freq_ngram_count_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33bda7c7-dee7-41b4-8650-719e5a34d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_full_feature_list(features,length):\n",
    "    '''\n",
    "        Build a full feature list using numpy array (very fast)\n",
    "        '''\n",
    "    full_feature_narray = np.zeros(length,)\n",
    "    full_feature_narray[features[:,0]] = features[:,1]\n",
    "    return full_feature_narray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d79141-db38-47fb-aec6-8d44816174b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
