{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c40918e0-018b-446b-b647-d012554db331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import re\n",
    "import numpy as np\n",
    "from operator import add\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a23c7e5-671c-4a40-b77d-082e5a66f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPCODE_PATTERN = re.compile(r'([\\s])([A-F0-9]{2})([\\s]+)([a-z]+)([\\s+])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fb7cce7-a005-4353-86a3-02da4e6a716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_folder_path, filenames, type):\n",
    "    myRDDlist = []\n",
    "    for filename in filenames:\n",
    "        new_rdd = sc.textFile(data_folder_path +\"/\"+ filename + type).map(lambda x: (filename,x)).groupByKey().map(lambda x: (x[0],' '.join(x[1])))\n",
    "        myRDDlist.append(new_rdd)\n",
    "    Spark_Full = sc.union(myRDDlist)\n",
    "    return Spark_Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "177ebffd-950f-4d18-9bc6-23807cededc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_label_pair(filenames_data_rdd,labels_rdd):\n",
    "    \"\"\"\n",
    "        This function matches the filename with label\n",
    "        \n",
    "        --input-------------------------------------\n",
    "        filenames_data_rdd : [<hash1>, <hash2>, ...]\n",
    "        labels_rdd : [label1, label2, ...]\n",
    "        \n",
    "        --output------------------------------------\n",
    "        filename_label_pair : [(<hash1>,<label1>), (<hash2>,<label2>), ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    id_filenames_rdd = filenames_data_rdd.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "    id_label_rdd = labels_rdd.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "    filename_label_pair = id_filenames_rdd.join(id_label_rdd).map(lambda x: x[1])\n",
    "    return filename_label_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd76bd95-be32-427d-95de-89936072be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_rdd, feature_name):\n",
    "    \"\"\"\n",
    "        This function extracts the required features\n",
    "        \n",
    "        --input-------------------------------------\n",
    "        file_rdd : [(<hash1>, <content1>), ...]\n",
    "        feature_name : str\n",
    "        \n",
    "        \n",
    "        --output------------------------------------\n",
    "        filename_label_pair : [(<hash1>,<feature1>), (<hash1>,<feature2>), ..., (<hashN>,<featureK>)]\n",
    "    \"\"\"\n",
    "    \n",
    "    if feature_name=='bytes':\n",
    "        return file_rdd.map(lambda x: (x[0],BYTES_PATTERN.findall(x[1]))).flatMapValues(lambda x:x)\n",
    "    elif feature_name=='segment':\n",
    "        return file_rdd.map(lambda x: (x[0],SEGMENT_PATTERN.findall(x[1]))).flatMapValues(lambda x:x)\n",
    "    elif feature_name=='opcode':\n",
    "        return file_rdd.map(lambda x: (x[0],OPCODE_PATTERN.findall(x[1]))).flatMapValues(lambda x:x).map(lambda x: (x[0],x[1][3]))\n",
    "    else:\n",
    "        return \"Invalid input!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4051459e-752e-460d-96c6-dff1ac371d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ngram(feature_rdd,start,end):\n",
    "    '''\n",
    "        --input-------------------------------------\n",
    "        feature_rdd : [(<hash1>,<feature1>), (<hash1>,<feature2>), ..., (<hashN>,<featureK>)]\n",
    "        \n",
    "        --output------------------------------------\n",
    "        Ngram_count : [((<hash>,<ngram feature>),cnt), ...]\n",
    "        '''\n",
    "    Ngram_list = []\n",
    "    for i in range(start,end):\n",
    "        Ngram_list.append(Ngram_feature(i, feature_rdd))\n",
    "    Ngram_count = sc.union(Ngram_list)\n",
    "    return Ngram_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80e3ab49-b2c4-4b0a-a761-ee63cd802b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ngram_feature(N, feature_rdd):\n",
    "    '''\n",
    "        Extract and count N-gram. Leave top 1000 n-gram features if it's 2-gram or more.\n",
    "        \n",
    "        Input:\n",
    "        feature_rdd : [(<hash1>,<feature1>), (<hash1>,<feature2>), ..., (<hashN>,<featureK>)]\n",
    "        \n",
    "        Output:\n",
    "        freq_ngram_count_rdd : [((<hash>,<ngram feature>),cnt), ...]\n",
    "        '''\n",
    "    feature_rdd = feature_rdd.groupByKey().map(lambda x: (x[0],list(x[1])))\n",
    "    df = spark.createDataFrame(feature_rdd).toDF(\"file_names\", \"features\")\n",
    "    ngram = NGram(n=N, inputCol=\"features\", outputCol=\"ngrams\")\n",
    "    ngramDataFrame = ngram.transform(df)\n",
    "    ngram_rdd = ngramDataFrame.rdd.map(tuple).map(lambda x: (x[0],x[2])).flatMapValues(lambda x: x)\n",
    "    ngram_count_rdd = ngram_rdd.map(lambda x: ((x),1)).reduceByKey(add)\n",
    "    freq_ngram_count_rdd = ngram_count_rdd\n",
    "\n",
    "    if not N == 1:\n",
    "        #[(<ngram feature>,cnt), ...]\n",
    "        topN_ngram_count_rdd = freq_ngram_count_rdd.map(lambda x: (x[0][1],x[1])).reduceByKey(add)\n",
    "        #[((<ngram feature>,cnt),index), ...]\n",
    "        topN_ngram_count_rdd = topN_ngram_count_rdd.sortBy(lambda x: x[1],ascending=False).zipWithIndex()\n",
    "        length = topN_ngram_count_rdd.count()\n",
    "        #top [(<ngram feature>,cntSum), ...]\n",
    "        topN_ngram_count_rdd = topN_ngram_count_rdd.filter(lambda x: x[1]<1000).map(lambda x: x[0])\n",
    "        #freq [(<ngram feature>,(<hash>,cnt)), ...]\n",
    "        freq_ngram_count_rdd = freq_ngram_count_rdd.map(lambda x: (x[0][1],(x[0][0],x[1])))\n",
    "        #[(<ngram feature>,(cntSum,(<hash>,cnt))), ...]\n",
    "        freq_ngram_count_rdd = topN_ngram_count_rdd.join(freq_ngram_count_rdd).map(lambda x: ((x[1][1][0],x[0]),x[1][1][1]))\n",
    "    \n",
    "    return freq_ngram_count_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33bda7c7-dee7-41b4-8650-719e5a34d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_full_feature_list(features,length):\n",
    "    '''\n",
    "        Build a full feature list using numpy array (very fast)\n",
    "        '''\n",
    "    full_feature_narray = np.zeros(length,)\n",
    "    full_feature_narray[features[:,0]] = features[:,1]\n",
    "    return full_feature_narray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d79141-db38-47fb-aec6-8d44816174b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_RF_structure(all_test_features_count,distinct_features_rdd):\n",
    "    '''\n",
    "        Build the data structure used for testing data\n",
    "        Leave only features that already appear in training\n",
    "        \n",
    "        Input:\n",
    "        all_test_features_count : [(<ngram feature>,(<hash>,cnt)), ...]\n",
    "        distinct_features_rdd : [(<ngram feature>,index), ...]\n",
    "        \n",
    "        Output:\n",
    "        all_test_features_count : [(<ngram feature>,((<hash>,cnt),index)), ...]\n",
    "        '''\n",
    "    #--[(<ngram feature>,(<hash>,cnt)), ...]-----------------------------------------\n",
    "    all_test_features_count = all_test_features_count.map(lambda x: (x[0][1],(x[0][0],x[1])))\n",
    "\n",
    "    #--[(<ngram feature>,(index,(<hash>,cnt))), ...]-----------------------------------------\n",
    "    all_test_features_count = all_test_features_count.leftOuterJoin(distinct_features_rdd).filter(lambda x: not x[1][1]==None)\n",
    "\n",
    "    #--[(<hash>,(index,cnt)), ...]-------------------------------------------------------\n",
    "    full_features_index_count_rdd = all_test_features_count.map(lambda x: (x[1][0][0],(x[1][1],x[1][0][1]))).groupByKey().map(lambda x: (x[0],np.asarray(list(x[1]),dtype=int)))\n",
    "\n",
    "    length = distinct_features_rdd.count()\n",
    "    #--[(<hash>,[cnt1, cnt2, ...]]), ...]-------------------------------------------------------\n",
    "    full_test_feature_count_rdd = full_features_index_count_rdd.map(lambda x: (x[0],Vectors.dense(list(build_full_feature_list(x[1],length)))))\n",
    "    \n",
    "    test_rdd = full_test_feature_count_rdd.map(lambda x: len(list(x[1])))\n",
    "    \n",
    "    return full_test_feature_count_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44b1f6ad-9b46-4082-b51b-85d47b8a669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_structure(all_features_count):\n",
    "    '''\n",
    "        Build the data structure used for training data\n",
    "        \n",
    "        Input:\n",
    "        all_features_count : [((<hash>,<ngram feature>),cnt), ...]\n",
    "        \n",
    "        Output:\n",
    "        full_feature_count_rdd : [((<hash1>,<label1>),[cnt1,cnt2,...]), ...]\n",
    "        '''\n",
    "    #--[(<ngram feature>,index), ...]------------------------------------------------\n",
    "    distinct_features_rdd = all_features_count.map(lambda x: x[0][1]).distinct().zipWithIndex()\n",
    "    length = distinct_features_rdd.count()\n",
    "\n",
    "    #--[(<ngram feature>,(<hash>,cnt)), ...]-----------------------------------------\n",
    "    all_features_count_rdd = all_features_count.map(lambda x: (x[0][1],(x[0][0],x[1])))\n",
    "\n",
    "    #--[(<hash>,(index,cnt)), ...]---------------------------------------------------\n",
    "    feature_id_count_rdd = distinct_features_rdd.join(all_features_count_rdd).map(lambda x: (x[1][1][0],(x[1][0],x[1][1][1])))\n",
    "\n",
    "    #--[(<hash>,[(index,cnt), ...]), ...]--------------------------------------------\n",
    "    feature_id_count_rdd = feature_id_count_rdd.groupByKey().map(lambda x: (x[0],np.asarray(list(x[1]),dtype=int)))\n",
    "\n",
    "    #--[(<hash>,DenseVector([cnt1,cnt2,...])), ...]-----------------------------------------------\n",
    "    full_feature_count_rdd = feature_id_count_rdd.map(lambda x: (x[0], Vectors.dense(list(build_full_feature_list(x[1],length)))))\n",
    "\n",
    "    test_rdd = full_feature_count_rdd.map(lambda x: len(list(x[1])))\n",
    "\n",
    "    return full_feature_count_rdd, distinct_features_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0351a38-e732-4422-b78e-75bbb05024e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indexed_df(full_train_feature_rdd):\n",
    "    '''\n",
    "        input: [(<hash1>,label1,[cnt1,cnt2,...]), ...]\n",
    "        '''\n",
    "    df = spark.createDataFrame(full_train_feature_rdd).toDF(\"name\",\"label\", \"features\")\n",
    "    \n",
    "    stringIndexer = StringIndexer(inputCol=\"name\", outputCol=\"indexed\")\n",
    "    si_model = stringIndexer.fit(df)\n",
    "    indexed_df = si_model.transform(df)\n",
    "    indexed_df.show()\n",
    "    return indexed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5be883f0-b82c-45a6-8556-0bb7846282cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF(indexed_df):\n",
    "    RF_model = RandomForestClassifier(numTrees=50, maxDepth=25, labelCol=\"label\")\n",
    "    td_new = change_column_datatype(indexed_df,\"label\",DoubleType)\n",
    "    model = RF_model.fit(td_new)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12c05191-a5e7-463f-873f-1a79e9d81453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_column_datatype(td,col_name,datatype):\n",
    "    td_new = td.withColumn(col_name, td[col_name].cast(datatype()))\n",
    "    return td_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c15aa37-b0e1-408e-bf04-8fe7ddbb891e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/06 20:19:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession.builder.master(\"master[*]\").appName(\"MalwareClassification\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22251fab-c3db-4bc7-b6a0-c12ef3ff6743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_asm_folder_path = './train/asm'\n",
    "train_filenames_array = np.genfromtxt('trainLabels.csv', delimiter=\",\", dtype=None, encoding=None)\n",
    "train_filenames_array = train_filenames_array[1:].tolist()\n",
    "train_filenames_list = [item[0] for item in train_filenames_array]\n",
    "train_filenames_list = [item.replace('\"', \"\") for item in train_filenames_list]\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "767a0d85-3d13-4185-9a40-a7e22ae0f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_asm_file_rdd = preprocess(data_asm_folder_path, train_filenames_list,\".asm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16996895-14ce-459d-8931-12cd4e64fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_list = [item[1] for item in train_filenames_array]\n",
    "train_labels_rdd = sc.parallelize(train_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd155b-b6f5-411c-ae6f-006b48a78790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
