{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687b39fb-3a0f-4dff-b7e3-014b319c8ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/07 22:29:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/01/07 22:29:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from operator import add\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "from pyspark.sql import functions as func\n",
    "\n",
    "sc = SparkContext()\n",
    "spark = SparkSession.builder.master(\"master[*]\").appName(\"MalwareClassification - classification\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56786d86-033d-427c-977d-606e9cb91be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "largeFeaturesDF = spark.read.options(header=True, inferSchema=True).csv(\"featureFrequency.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54019254-007d-45f5-b128-008657b8d4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-----+---+----+----+-------+-----+---+----+---+----+----+---+---+---+---+---+----+----+----+----+-----+-----+---+---+----+----+----+---+---+---+----+----+----+\n",
      "|                  ID|add|align|and|call| cmp|     db|   dd|dec|  dw|fld|fstp|imul|inc| jb|jge| jl|jle| jmp| jnz|  jz| lea|  mov|movzx|mul| or| pop|push|retn|shl|shr|sub|test|xchg| xor|\n",
      "+--------------------+---+-----+---+----+----+-------+-----+---+----+---+----+----+---+---+---+---+---+----+----+----+----+-----+-----+---+---+----+----+----+---+---+---+----+----+----+\n",
      "|1UJIV2ntdkuWgzvFr4pm|129|  271|100|  67|  94|  50789|  397| 44|  20|  9|   9|  15| 68| 12| 15| 25|  0|  62|  33|  39|  86|  552|    7|  8| 61|  99| 209|   6| 50| 11| 68|  47|  23|  65|\n",
      "|2UcBWPt6mf8dohDsYxOR|243|  422|223| 430| 485| 870295|  253| 95|   0|  3|   4|  55|155| 77| 28| 51|  9| 385| 318| 451| 412| 2232|  328| 50|295| 528|1042|  71| 84| 90|336| 504|   0| 519|\n",
      "|2bf65CdkZTUGXjzq9KNJ|772|  422|229| 541| 517|  82120|  220| 96|   1| 72| 133| 206|208|128| 30| 55| 35| 200| 145| 256| 396| 4473|   59| 80|185| 604|1219| 101|116|142|477| 244|   1| 460|\n",
      "|3Mlb2WXYpOiJLeEv60nr| 65|   37|  2| 127|   3|   1038|   56|  2|   0|  0|   0|   0|  0|  1|  0|  0|  0|   0|   2|   1|  15|   56|    0|  0|  2|  99| 121|   0|  0|  0| 23|   1|   0|  46|\n",
      "|3PmaDdlEYWrhG6e480xp|189|   61|  5| 171|   5|   1042|   70|  2|   0|  0|   0|   0|  0|  0|  0|  0|  0|   1|   7|   1|  42|  150|    0|  0|  2| 167| 355|   0|  0|  0| 62|   7|   1|   8|\n",
      "|3rf7KJiPd8hIZ645wBVN|127|   61|351| 178| 167|   2503|  118|  0|   4|  0|   0|   0|  1|101|  0| 54|  3|   2|  70| 102| 291|   36|  237|  0|204|  63| 389|  25|  0|  0|172| 266|   0| 387|\n",
      "|41wOoxNbCqTKQImafyDJ|347|  477|315|2268| 502|  13066| 1605| 40|   7| 16|  26|   2| 92| 13|  7|  5|  4| 836| 298| 796| 696| 6316|   33|  0| 38|1000|2917| 312|  3|  2| 88| 676|   0| 623|\n",
      "|4ZVUhT8gufvReG2XS51w|420|   51| 13| 204| 181|   4326|  101|  6|   2|  0|   2|   0|  4|  3|  3|  5|  7|  74|  67|  78|  16| 2535|   23|  0| 19|  31| 386|   2| 30|  1|366|   0|   3|  16|\n",
      "|5QPsHaRAEIt7ibDdVy3F|279|   49|132| 248| 535|  34391|   86| 53|  15|  0|   0|  40| 99| 29| 21| 55| 28| 135| 229| 346| 268| 2214|  322|  0| 48| 313| 813|  91| 50| 19|114| 303|  13| 190|\n",
      "|5YcQadwmPLqg72tvW8iS|622| 4970|142| 116| 129|  31098|39960|144|5421|  9|   3|  22|155|  9|  6|  8|  7|1075|  13|  13|  55|  764|    1|  1|145| 237| 537|  82| 18| 12|169| 100| 135| 152|\n",
      "|6TwA8C7RumbeKOnN5XZl|122|   43|  3| 168|   8|   2352|   16|  1|   0|  0|   0|   0|  1|  3|  0|  0|  0|   0|  11|   2|  45|  194|    0|  0| 10| 185| 451|   0|  0|  0| 63|  10|   1|  11|\n",
      "|6a3bdiu9jyrXPQHI8lpJ|221|  275|152| 341| 204|2262345|  214| 54|   0| 28|  28|  37| 66| 71| 14| 25|  0|  95|  53| 103| 273| 2016|   78| 49|170| 260| 611|  61| 56| 65|225| 104|   0| 215|\n",
      "|8sNQwJpT43ohCx2MjHaD|314|   67| 21| 171| 154|   6391|  114|  9|   1|  0|   0|   3|  9|  2|  5|  3|  7|  70|  53|  71|  16| 1867|   23|  0| 12|  30| 254|   2| 16|  2|282|   1|   7|  15|\n",
      "|9LWNpGBmUctnVraSis5j|421|   57| 11| 204| 180|   5353|  100| 11|  11|  1|   0|   1| 12|  3|  4|  4|  6|  73|  68|  77|  17| 2542|   23|  0| 16|  33| 384|   2| 33|  1|360|   5|   9|  15|\n",
      "|AnyrEaNMze5xH7PhCobc|442|  416|691|3866|1365|   1767| 4047|146|   7|  2|   8|  11|251| 58|321|369| 39| 952|1049|1311|2814|10858|  116|  0| 60|2122|6855| 730| 27| 12|554|1735|   1| 752|\n",
      "|BT7h2e3AcNWn6UvO5CIx|962|  125|345|3778|1648|    589|   94| 51|  59|  0|   0|  15|389| 65| 27| 88| 69| 662| 932|1779|1119| 6407|  169|  7|116|2346|6827|  51| 82|121|418|1469|   0|1005|\n",
      "|Bhs2Pa0NyQ8ArJW5fq9R|713|  299|377|1477| 885|   1064|  141| 91|   4|  0|   0|   9|220| 50|  4| 19| 12| 343| 454| 607| 632| 4822|   50| 48|160| 859|4078| 193| 23| 69|197| 345|   1| 437|\n",
      "|C1okxVtRdfUWIXY9G3s8|585|  127|196|  15| 482| 387840|  626|  1|  63|  0|   0|   0|  1|  0|  0|  0|  0|   2| 306| 276|   2|  790|    0|  0|152|  15|  36|   2|  0|  1|611|  98|   0| 151|\n",
      "|CV7DFk6tUNpbyJcwEKsx|466|   22| 61|  41|  25| 262904|   35|  7|  32|  0|   1|   0| 53|  2|  0|  0|  0|  53|  20|  18|  69|  432|    2|  1| 61| 155| 456|   8|  5|  6|244|  10|  49| 274|\n",
      "|CVcysi9h3kTI1gXj0qHm|135|   63|262|  65|  64|    758|  129|  0|   0|  0|   0|   0|  0| 23|  0| 18|  2|   1|  27|  20| 132|   26|  103|  0|129|  75| 254|  17|  0|  0| 55| 135|   0| 119|\n",
      "+--------------------+---+-----+---+----+----+-------+-----+---+----+---+----+----+---+---+---+---+---+----+----+----+----+-----+-----+---+---+----+----+----+---+---+---+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/07 22:35:28 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# reducedFeatureDF = largeFeaturesDF.drop(largeFeaturesDF.select()(largeFeaturesDF.agg({'db': 'mean'}).collect()[0][0] < 30))\n",
    "columnList = largeFeaturesDF.columns\n",
    "columnsToDrop = [i for i in columnList[1:] if (largeFeaturesDF.agg({i: 'mean'}).collect()[0][0]) < 30]\n",
    "reducedFeatureDF = largeFeaturesDF.drop(*columnsToDrop)\n",
    "reducedFeatureDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b03b5a8a-44f8-498b-965b-507745915fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedFeatureDF.toPandas().to_csv(\"reducedFeatures.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e7405d-7645-4a6a-b5df-715e5df98988",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedFeatureDF = spark.read.options(header=True, inferSchema=True).csv(\"reducedFeaturesWithClass.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bba6628-6e21-4dff-9d20-a31383e2b5f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Output column features already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11308/1220371305.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfeaturecolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreducedFeatureDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0massembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeaturecolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreducedFeatureDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreducedFeatureDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mreducedFeatureDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Output column features already exists."
     ]
    }
   ],
   "source": [
    "featurecolumns = reducedFeatureDF.columns\n",
    "assembler = VectorAssembler(inputCols=featurecolumns[1:], outputCol=\"features\")\n",
    "reducedFeatureDF = assembler.transform(reducedFeatureDF)\n",
    "reducedFeatureDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ae52dce-b45c-41bb-8fb6-d840aed9e77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 7650\n",
      "Test Dataset Count: 3210\n"
     ]
    }
   ],
   "source": [
    "train, test = reducedFeatureDF.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "341ae37c-b5b5-4b6a-ab2f-be56084be655",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'Class', maxDepth=10, maxBins=32, numTrees=10)\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1242b947-9193-43e8-af7a-43dfd2683578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|Class|prediction|\n",
      "+-----+----------+\n",
      "|    2|       2.0|\n",
      "|    2|       2.0|\n",
      "|    6|       6.0|\n",
      "|    3|       3.0|\n",
      "|    8|       8.0|\n",
      "|    1|       1.0|\n",
      "|    2|       2.0|\n",
      "|    3|       3.0|\n",
      "|    2|       2.0|\n",
      "|    3|       3.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"Class\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9dfa846d-f488-4585-8144-68040eefa8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9949209144215765\n",
      "Test Error = 0.0050790855784235145\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be06c43-5b06-45c7-9fb9-7ae7f193dbb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
